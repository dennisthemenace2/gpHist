\documentclass{article}

\usepackage[utf8]{inputenc}

\usepackage[english]{babel} 
\selectlanguage{english}

\usepackage{cite}
\usepackage{float}
\usepackage{amssymb,amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{capt-of}
\usepackage{bm}

\usepackage{mathtools}

\usepackage{graphicx}


\usepackage{subfig}


\begin{document}
\SweaveOpts{concordance=TRUE}

\section*{gpHist R package}


\section{Introduction}


Gaussian process is a powerfull tool to approximate data and predict new sample means and their variance. However, training of a Gaussian process scales cubically in terms of runtime and quadradically in terms of memory usage. 

An alternative in order to reduce the memory and computational burden, is the use of Gaussian process with a histogram intersection kernel (HIK). This particular kernel has been utilized for computer vision due its fast learning, classification and linear memory requirements~\cite{Wu2011,Wu2010,Maji2008}. Initially used as a kernel for support vector machines, it can also be used for the Gaussian process. Although the kernel only provides a piecewise linear approximation of the true function, its capability for large-scale Gaussian process~\cite{Rodner2012, Rodner2016} inference appears attractive for the use as surrogate function.


\subsection{Gaussian Process Regression}

For the approximation of the function, we consider the regression problem $y=f(\mathbf{x})+\epsilon \in \mathbb{R}$. In this function, it is assumed that the observed data $y_i$ is generated by an unknown function $f(\mathbf{x}_i)$, and potentially corrupted by additional independent noise $\epsilon \sim \mathcal{N}(0, \sigma_y^2 )$. 
A Gaussian process~\cite{Rasmussen} defines a prior over functions that could have created the observed data with mean 0 and covariance given by $\Sigma_{ij} = \kappa(x_i , x_j )$. The covariance matrix is built by the kernel function $\kappa$ which describes the similarity among samples. 

For the prediction of new data points, the posterior predictive distribution of the new data sample $\mathbf{X}_*$ is calculated by marginalizing over all possible functions.


The posterior predictive distribution of the corresponding function value $y_*=f(\mathbf{x}_*)$ is a Gaussian with mean and variance given by,

\begin{align*}
p(\mathbf{f_*}\mid \mathbf{X_*}, \mathbf{X}, \mathbf{y}) &=  \mathcal{N} (\mathbf{f_*}\mid \bm{\mu_*} , \mathbf{\Sigma_*} ), \\
  \bm{\mu_*}  &=   \mathbf{K_*^\textit{T }}  \mathbf{K_\text{y}^{-\text{1}}} \mathbf{y}, \\
\mathbf{\Sigma_*} &= \mathbf{K_{**}} - \mathbf{K_*^\textit{T }} \mathbf{K_\text{y}^{-\text{1}} }\mathbf{K_*},
\end{align*} 
%$K + \sigma_y^2 = K_y$

where $\mathbf{K_y} = \kappa(\mathbf{X}, \mathbf{X}) + \sigma^2_y \mathbf{I} \in \mathbb{R}^{N\times N}$, $\mathbf{K}_* = \kappa(\mathbf{X}, \mathbf{X}_*) \in \mathbb{R}^{N\times N_*}$, and $\mathbf{K}_{**} = \kappa(\mathbf{X}_* , \mathbf{X}_*) \in \mathbb{R}^{N_*\times N_*}$. For the Gaussian process regression, all these computations can be executed in closed form.
However, the creation of the $N\times N$ kernel matrix requires $\mathcal{O}(N^2)$ space and inversion of this matrix $\mathcal{O}(N^3)$ time. These requirements practically limit the use of Gaussian process to data sets of size $\mathcal{O}(10^4)$.


\subsection{Histogram Intersection Kernel}


To reduce the computational effort of the Gaussian process, the histogram intersection kernel is utilized. The histogram intersection kernel is defined as,

\begin{equation}
\kappa_{\text{HIK}} (\bm{x}_i,\bm{x}_j)= \sum_{d=1}^{D} \text{min}(\bm{x}_i(d),\bm{x}_j(d) ). 
\end{equation}

It has been shown that properties of this kernel allow to speed up model learning and prediction of new data samples~\cite{Wu2011,Wu2010,Maji2008}. The multiplication of the kernel matrix $\mathbf{K}$ by an arbitrary vector $v$ can be done without explicitly creating the kernel matrix, which enables sub quadratical calculation of the matrix-vector product. This alleviates computational and memory storage cost during the training of the model. Furthermore, the prediction of new data samples scales sub-linear, which improves prediction time respectively.

However, these considerations only apply when each dimension of the kernel matrix is sorted. For illustration of these properties, we first consider the prediction of a new data sample given the matrix $\mathbf{K_*}$ and $\bm{\alpha} =  \mathbf{K_\text{y}^{-\text{1}}} \mathbf{y}$,

\begin{align*}
\mathbf{K}^T_* \bm{\alpha} &= \sum^N_{i=1} \bm{\alpha}(i) \sum^D_{d=1}\text{min}(\bm{x}_i(d), \bm{x}_*(d) ), \\
 &= \sum^D_{d=1} \left ( \mkern31mu \sum_{\mathclap{\substack{i: \bm{x}_i(d)<\bm{x}_*(d)}} }   \bm{\alpha}(i)\bm{x}_i(d) +    \bm{x}_*(d)\sum_{\makebox[0pt]{$\scriptstyle j:\bm{x}_j(d)\geq \bm{x}_*(d)$ }} \bm{\alpha}(j) \right ).
\end{align*}


For each dimension, the summation can be separated into two parts. The first part consists of the samples $\mathbf{x}$ whose values are smaller than the value of the new sample to predict $\mathbf{x_*}$. For the calculation of this term, the values of $\bm{\alpha}$ have to be multiplied by the values of $\mathbf{x}$ and are summed up. The second term consists of all samples where $\mathbf{x_*}$ is smaller than or equal to $\mathbf{x}$. For this part of the kernel matrix $\mathbf{K_*}$, all values will be equal to $\mathbf{x_*}$. This allows to sum the remaining values of alpha and multiply the result by the value of $x_*$.
Similar observations apply to the multiplication of the kernel matrix $\mathbf{K}$ with an arbitrary vector $\bm{v}$.
The following equation illustrates the calculation of one value of the kernel vector product for one dimension, 
\begin{align*}
(\mathbf{K} \bm{v})_i &=   \sum^N_{j=1} \bm{v}(j) \cdot \kappa (\bm{x}_{j}, \bm{x}_{i} ). \\
\end{align*}

For the calculation of each value of the matrix-vector product, the kernel value will be $\mathbf{x}_j$ for all samples smaller than $\mathbf{x}_i$ and $\mathbf{x}_j$ for all remaining values. This allows the efficient multiplication of the kernel matrix with an arbitrary vector when the samples are sorted for each dimension. Therefore the kernel matrix has not to be stored, which reduces the memory requirements from $\mathcal{O}(N^2)$ to $\mathcal{O}(2ND)$ assuming the matrix with the sort indices is stored for later use.


The definition of the histogram intersection kernel can be further generalized to increase its flexibility.
Boughorbel et al. (2005)\cite{Boughorbel2005} have shown that any positive valued function $g(\cdot)$ can be applied to the data and the kernel remains positive definite. This allows transformation of the data and is often referred to as the generalized histogram intersection kernel:

\begin{equation}
\kappa_{\text{GHIK}} (\bm{x}_i,\bm{x}_j)= \sum_{d=1}^{D} \text{min}(g(\bm{x}_i(d)),g(\bm{x}_j(d)) ) .
\end{equation}

As transformation function, we utilize the exponential transformation:

\begin{equation}
g_{\text{exp}} (\bm{x}(d))= \frac{\text{exp}(\eta \left|  \bm{x}(d) \right| )-1 }{\text{exp}(\eta)-1} .
\end{equation}

This transformation retains the order of the samples and therefore the fast evaluation of the kernel vector product can still be applied.
For each dimension, a different parameter $\bm{\eta}(d)$ is used in order to individually weight each dimension.

\subsection{Utilized approximations}

The implementation of the Gaussian process with HIK mostly follows the implementation of Rodner et al. (2016). %~\cite{Rodner2016}.
For the prediction of the mean of new samples values, the values of the vector $\bm{\alpha}$ are required. The $\bm{\alpha}$ values are estimated using conjugate gradient descent, which allows to never estimate the inverse of the matrix $\mathbf{K_y}$. In the absence of round-off errors, this would allow to obtain the exact solution after $N$ iterations. In practice, the algorithm can be stopped significantly earlier when the norm of the residual drops below a specified threshold.

For the estimation of the sample variance, which is needed for the acquisition function of the Gaussian process, the inverse of the matrix $\mathbf{K_y}$ is required. 
However, to reduce the computational demand, the variance can be approximated with a lower number of eigenvectors. For the estimation of the eigenvalues, the Lanczos algorithm~\cite{Lanczos1950} with full reorthogonalization is used. To estimate the eigenvalues from the resulting tridiagonal matrix, the strum sequences are estimated and bounded by bisection. The corresponding eigenvectors are estimated using inverse iteration. Although this requires inversion of the kernel matrix, this is achieved by applying the conjugate gradient descent again. The determinant of the kernel matrix, likelihood, and prediction variance are approximated using the approach presented in Rodner et al. (2016). 



\begin{align*}
\sigma_*^2 \leq \mathbf{K}_{**} -   \sum^k_{i=1} \frac{1}{\xi_i} \nu(i)^2  - \frac{1}{\xi_{k+1}} (||\mathbf{K}{*}||^2 -\sum^k_{i=1}\nu(i)^2 ) \\
\end{align*}


the coarse approximaation
\begin{align*}
\sigma_*^2 \leq \mathbf{K}_{**} -  \frac{1}{\xi_{1}} ||\mathbf{K}_*||^2  \\
\end{align*}


\begin{align*}
||\mathbf{K}_*||^2 = \mathbf{K}_*^T \mathbf{K}_* =  \sum^N_{i=1}  \sum^D_{d=1} \min(\bm{x}_*(d) ,\bm{x}_i(d) ) 	   \\
\end{align*}



The hyperparameters that are required for the exponential transformation and shift of the data are estimated using the downhill simplex algorithm~\cite{Nelder1965}, which is a gradient free optimization algorithm. This avoids estimation of the gradient, which would require an inversion of the kernel matrix.



\subsection{Implementation details}

The kernel matrix resulting from the HIK can be multiplied with an arbitrary vector without creating the kernel matrix.
The vector of data samples $X=\begin{bmatrix}1\\2\\3\end{bmatrix}$ and the vector $\nu=\begin{bmatrix}2\\4\\6\end{bmatrix}$ the kernel matrix from the vector would be $\kappa_{min}=\begin{bmatrix}
 1& 1 &1 \\ 
 1&  2&2 \\ 
 1&2  &3 
\end{bmatrix}$, however, we know what the kernel would look like thats why we can calculate this without creating the kernel matrix as shown below.

$
\begin{bmatrix}1\\2\\3\end{bmatrix}
\times
\begin{bmatrix}2\\4\\6\end{bmatrix}
=
\begin{Bmatrix}
2 &2  &2 \\ 
4 &8  &8 \\ 
6 &12  &18 
\end{Bmatrix}
=
\begin{bmatrix}12\\ 22\\28\end{bmatrix}
$

 
If we consider we the intermediate result of the multiplication, we notice that the multiplied vector is the diagonal. 
This further improves us to speed up the calculation. However, it assumes that the values are ordered.


\bibliography{bib}
\bibliographystyle{plain}




\end{document}
