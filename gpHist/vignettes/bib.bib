@article{Nelder1965,
abstract = {A method is descipted for minimization of a function of n variables, which depends on the comparison of function values at the (n+1) vertices of the general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the hessian matrix on the neighbourhood of the minimum, needed in statistical estimation problems.},
author = {Nelder, J. A. and Mead, R.},
doi = {10.1093/comjnl/7.4.308},
file = {:home/dennis/Downloads/bayesoptim/7-4-308.pdf:pdf},
isbn = {9781605580852},
issn = {0010-4620},
journal = {The Computer Journal},
mendeley-groups = {genticalgorithm/stuffforpaper},
number = {4},
pages = {308--313},
title = {{A Simplex Method for Function Minimization}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/7.4.308},
volume = {7},
year = {1965}
}


@article{Lanczos1950,
abstract = {The present investigation designs a systematic method for finding the latent roots and the principal axes of a matrix, without reducing the order of the matrix. It is characterized by a wide field of applicability and great accuracy, since the accumulation of rounding errors is avoided, through the process of "minimized iterations". Moreover, the method leads to a well convergent successive approximation procedure by which the solution of integral equations of the Fredholm type and the solution of the eigenvalue problem of linear differential and integral operators may be accomplished.},
author = {Lanczos, C.},
doi = {10.6028/jres.045.026},
file = {:home/dennis/Downloads/V45.N04.A01.pdf:pdf},
issn = {0091-0635},
journal = {Journal of Research of the National Bureau of Standards},
mendeley-groups = {genticalgorithm},
number = {4},
pages = {255},
title = {{An iteration method for the solution of the eigenvalue problem of linear differential and integral operators}},
url = {http://nvlpubs.nist.gov/nistpubs/jres/045/jresv45n4p255{\_}A1b.pdf},
volume = {45},
year = {1950}
}

@manual{RCoreDevelopmentTeam2014,
address = {Vienna, Austria},
author = {{R Core Development Team}},
isbn = {3-900051-07-0},
mendeley-groups = {prop},
organization = {R Foundation for Statistical Computing},
title = {{R: a language and environment for statistical computing, 3.1.2 ed.}},
url = {http://www.r-project.org/},
year = {2014}
}

@misc{Rodner2016,
author = {Rodner, Erik and Freytag, Alexander and Bodesheim, Paul and Fr{\"{o}}hlich, Bj{\"{o}}rn and Denzler, Joachim},
booktitle = {International Journal of Computer Vision},
doi = {10.1007/s11263-016-0929-y},
file = {:home/dennis/Downloads/genetic stuff/hist/art{\%}3A10.1007{\%}2Fs11263-016-0929-y.pdf:pdf},
issn = {15731405},
keywords = {Gaussian processes,Hyperparameter optimization,Large-scale learning,Visual recognition},
mendeley-groups = {genticalgorithm/histogram kernel},
pages = {1--28},
publisher = {Springer US},
title = {{Large-Scale Gaussian Process Inference with Generalized Histogram Intersection Kernels for Visual Recognition Tasks}},
url = {"http://dx.doi.org/10.1007/s11263-016-0929-y},
year = {2016}
}


@article{Boughorbel2005,
author = {Boughorbel, Sabri and Tarel, Jean--Philippe and Boujemaa, Nozha},
doi = {10.1109/ICIP.2005.1530353},
file = {:home/dennis/Downloads/genetic stuff/hist/jpt-icip05.pdf:pdf},
isbn = {0-7803-9134-9},
journal = {{\{}IEEE{\}} International Conference on Image Processing},
mendeley-groups = {genticalgorithm/histogram kernel},
number = {October},
pages = {III----161},
title = {{Generalized Histogram Intersection Kernel for Image Recognition}},
volume = {3},
year = {2005}
}


@article{Wu2011,
abstract = {Common visual codebook generation methods used in a bag of visual words model, for example, k-means or Gaussian Mixture Model, use the Euclidean distance to cluster features into visual code words. However, most popular visual descriptors are histograms of image measurements. It has been shown that with histogram features, the Histogram Intersection Kernel (HIK) is more effective than the Euclidean distance in supervised learning tasks. In this paper, we demonstrate that HIK can be used in an unsupervised manner to significantly improve the generation of visual codebooks. We propose a histogram kernel k-means algorithm which is easy to implement and runs almost as fast as the standard k-means. The HIK codebooks have consistently higher recognition accuracy over k-means codebooks by 2-4{\%} in several benchmark object and scene recognition data sets. The algorithm is also generalized to arbitrary additive kernels. Its speed is thousands of times faster than a naive implementation of the kernel k-means algorithm. In addition, we propose a one-class SVM formulation to create more effective visual code words. Finally, we show that the standard k-median clustering method can be used for visual codebook generation and can act as a compromise between the HIK / additive kernel and the k-means approaches.},
author = {Wu, Jianxin and Tan, Wc and Rehg, Jm},
file = {:home/dennis/Downloads/genetic/stuffforpaper/wu11b.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {additive kernel,histogram intersection kernel,visual codebook},
mendeley-groups = {genticalgorithm/histogram kernel},
pages = {3097--3118},
title = {{Efficient and effective visual codebook generation using additive kernels}},
url = {http://dl.acm.org/citation.cfm?id=2078205},
volume = {12},
year = {2011}
}
@article{Wu2010,
abstract = {Histograms are used in almost every aspect of computer vision, from$\backslash$nvisual descriptors to image representations. Histogram Intersection$\backslash$nKernel (HIK) and SVM classifiers are shown to be very effective in$\backslash$ndealing with histograms. This paper presents three contributions$\backslash$nconcerning HIK SVM classification. First, instead of limited to integer$\backslash$nhistograms, we present a proof that HIK is a positive definite kernel$\backslash$nfor non-negative real-valued feature vectors. This proof reveals$\backslash$nsome interesting properties of the kernel. Second, we propose ICD,$\backslash$na deterministic and highly scalable dual space HIK SVM solver. ICD$\backslash$nis faster than and has similar accuracies with general purpose SVM$\backslash$nsolvers and two recently proposed stochastic fast HIK SVM training$\backslash$nmethods. Third, we empirically show that ICD is not sensitive to$\backslash$nthe C parameter in SVM. ICD achieves high accuracies using its default$\backslash$nparameters in many datasets. This is a very attractive property because$\backslash$nmany vision problems are too large to choose SVM parameters using$\backslash$ncross-validation.},
author = {Wu, Jianxin},
doi = {10.1007/978-3-642-15552-9_40},
file = {:home/dennis/Downloads/genetic/stuffforpaper/eccv{\_}2010.pdf:pdf},
isbn = {3642155510},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {genticalgorithm/histogram kernel},
number = {PART 2},
pages = {552--565},
title = {{A fast dual method for HIK SVM learning}},
volume = {6312 LNCS},
year = {2010}
}

@inproceedings{Maji2008,
abstract = {Straightforward classification using kernelized SVMs re-quires evaluating the kernel for a test vector and each of the support vectors. For a class of kernels we show that one can do this much more efficiently. In particular we show that one can build histogram intersection kernel SVMs (IKSVMs) with runtime complexity of the classifier logarith-mic in the number of support vectors as opposed to linear for the standard approach. We further show that by precom-puting auxiliary tables we can construct an approximate classifier with constant runtime and space requirements, independent of the number of support vectors, with negli-gible loss in classification accuracy on various tasks. This approximation also applies to 1 − $\chi$ 2 and other kernels of similar form. We also introduce novel features based on a multi-level histograms of oriented edge energy and present experiments on various detection datasets. On the INRIA pedestrian dataset an approximate IKSVM classifier based on these features has the current best performance, with a miss rate 13{\%} lower at 10 −6 False Positive Per Window than the linear SVM detector of Dalal {\&} Triggs. On the Daimler Chrysler pedestrian dataset IKSVM gives comparable ac-curacy to the best results (based on quadratic SVM), while being 15× faster. In these experiments our approximate IKSVM is up to 2000× faster than a standard implementa-tion and requires 200× less memory. Finally we show that a 50× speedup is possible using approximate IKSVM based on spatial pyramid features on the Caltech 101 dataset with negligible loss of accuracy.},
author = {Maji, Subhransu and Berg, Alexander C and Malik, Jitendra},
booktitle = {Slides},
doi = {10.1109/CVPR.2008.4587630},
file = {:home/dennis/Downloads/genetic stuff/hist/mbm08cvpr.pdf:pdf},
isbn = {9781424422432},
issn = {10636919},
mendeley-groups = {genticalgorithm/histogram kernel},
pages = {1--8},
title = {{Classification using Intersection Kernel Support Vector Machines is Efficient Classification using Intersection Kernel Support Vector Machines is Efficient}},
year = {2008}
}

@inproceedings{Rodner2012,
author = {Rodner, Erik and Freytag, Alexander and Bodesheim, Paul and Denzler, Joachim},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33765-9_7},
file = {:home/dennis/Downloads/genetic stuff/hist/Rodner12{\_}LGP.pdf:pdf},
isbn = {9783642337642},
issn = {03029743},
keywords = {Bayesian Modeling,Histogram Intersection Kernels,Hyperparameter Optimization,Large-scale Gaussian Processes},
mendeley-groups = {genticalgorithm/histogram kernel},
number = {PART 4},
pages = {85--98},
title = {{Large-scale Gaussian process classification with flexible adaptive histogram kernels}},
volume = {7575 LNCS},
year = {2012}
}


@book{Rasmussen,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, CE. and Williams, CKI.},
  pages = {248},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  address = {Cambridge, MA, USA},
  month = jan,
  year = {2006},
  month_numeric = {1}
}
